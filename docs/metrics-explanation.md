# Metrics Explanation â€” GitHub User Analytics V2

**Version:** 2.0
**Date:** 2025-11-17
**Status:** Conceptual - See [VERIFICATION_REPORT.md](./VERIFICATION_REPORT.md) for API limitations

**âš ï¸ API LIMITATIONS WARNING:**

This document describes metrics v2.0 concepts. However, some GitHub GraphQL API limitations affect implementation:

- âŒ **commit.additions/deletions** - NOT available in GitHub GraphQL API
- âœ… **pullRequest.additions/deletions** - Available (but requires fetching all PRs = slow + rate limit)
- âŒ **commit.author.email** - NOT available (privacy reasons)
- âœ… **commit.author.user** - Available (null if email not linked to GitHub account)

**Recommendation:** Use metrics v1.0 (based on `src/lib/authenticity.ts`) which uses only available API fields.

---

## ðŸ“Š Overview

This document explains the GitHub User Analytics metrics system:
- **What** each metric measures
- **How** it's calculated (formulas)
- **Why** it matters for evaluating developers
- **What** the benchmark ranges mean
- **How** to detect fraud and fake activity

---

## ðŸŽ¯ Main Question

**"Can this person bring value to our team?"**

This question breaks down into 4 sub-questions:

1. **Do they work regularly?** â†’ **Activity Score**
2. **Do people use their work?** â†’ **Impact Score**
3. **Do they write reliable code?** â†’ **Quality Score (Engineering Maturity)**
4. **Are they growing?** â†’ **Growth Score (Learning Trajectory)**

Plus: **Fraud Detection** to identify fake GitHub activity patterns.

---

## ðŸ›¡ï¸ FRAUD DETECTION SYSTEM

### Purpose

Detect GitHub farming and fake activity created using tools that manipulate `GIT_AUTHOR_DATE` and `GIT_COMMITTER_DATE`.

### Fraud Score: 0-100

**Formula:**
```
Fraud Score = (empty_commits_ratio Ã— 30) +
              (perfect_pattern_score Ã— 25) +
              (temporal_anomaly Ã— 20) +
              (mass_commits_ratio Ã— 15) +
              (fork_without_changes Ã— 10)
```

**Range:** 0-100 (where 100 = 100% suspicion of fraud)

### Detection Signals

| Signal | How to Detect | Weight | Criticality |
|--------|---------------|--------|-------------|
| **Empty commits** | `additions + deletions == 0` | 30% | ðŸ”´ High |
| **Backdated commits** | Commits before account creation | Auto-flag | ðŸ”´ Critical |
| **Bot patterns** | Perfect daily commits at same time | 25% | ðŸŸ¡ Medium |
| **Temporal anomalies** | Commits outside usual working hours | 20% | ðŸŸ¢ Low |
| **Mass commits** | >1000 lines in single commit | 15% | ðŸŸ¡ Medium |
| **Fork farming** | Many forks with no modifications | 10% | ðŸŸ¡ Medium |
| **Multiple emails** | >10 different emails in commits | Flag only | ðŸŸ¡ Medium |
| **No GPG signing** | All commits unverified | Flag only | ðŸŸ¢ Low |

### Benchmark Ranges

| Score | Level | Interpretation | Action |
|-------|-------|----------------|--------|
| 0-19 | Clean | No suspicious patterns | âœ… Safe to hire |
| 20-39 | Low Risk | Minor irregularities | âš ï¸ Monitor |
| 40-59 | Medium Risk | Multiple red flags | ðŸŸ¡ Investigate further |
| 60-79 | High Risk | Significant fraud indicators | ðŸ”´ Major concern |
| 80-100 | Critical | Likely fake profile | âŒ Do not hire |

### Example Output

```
âš ï¸ Fraud Risk: 35% (Medium)

Issues detected:
â€¢ 15% of commits are empty (4.5 points)
â€¢ 30% commits outside working hours (6.0 points)
â€¢ 8 different email addresses used (flag)
â€¢ 3 unmodified forks in profile (3.0 points)

Total Score: 35/100 â†’ Medium Risk

Recommendations:
âœ“ Use GPG signing for verified commits
âœ“ Clean up inactive forks
âœ“ Add meaningful commit messages
```

---

## ðŸ“Š METRIC 1: Activity Score â†’ Productivity Signal

### Purpose

Measures **how productively** the developer works, not just "how many commits". Focuses on real code output and work patterns.

**Range:** 0-100 points

### Formula

```
Activity = Code Throughput (35) +
           Consistency & Rhythm (25) +
           Collaboration (20) +
           Project Focus (20)
```

### Components

#### A. Code Throughput (0-35 points)

**What it measures:** Real output via **lines changed** in **merged PRs**.

**Why:** 1 commit can be 1 line or 10,000 lines. Lines changed is a more honest indicator.

**Calculation:**
```typescript
linesChanged = sum(mergedPRs.map(pr => pr.additions + pr.deletions))
linesPerMonth = linesChanged / 3 // Last 3 months

Scoring:
â€¢ 0-1000 lines/month     â†’ 0-15 points (Low)
â€¢ 1000-5000 lines/month  â†’ 15-25 points (Moderate)
â€¢ 5000-15000 lines/month â†’ 25-35 points (High)
â€¢ 15000+ lines/month     â†’ 35 points (Very High)

Penalty: -10 points if >50% of code in mass commits (>1000 lines)
```

**Anti-fake protection:**
- Ignore repos with one massive commit (90% code in single PR = likely clone)
- Flag if average PR size >1000 lines

---

#### B. Consistency & Rhythm (0-25 points)

**What it measures:** Regular work patterns and commit streaks.

**Calculation:**
```typescript
activeWeeks = count weeks with â‰¥1 commit (last 12 months)
longestStreak = max consecutive active weeks

Base score:
â€¢ 40+ weeks out of 52 â†’ 20-25 points (High)
â€¢ 20-39 weeks â†’ 10-19 points (Moderate)
â€¢ <20 weeks â†’ 0-9 points (Low)

Bonus: +5 points if longestStreak â‰¥ 26 weeks (half year)
```

**Temporal Pattern Analysis (Anti-bot):**
```typescript
// Build hour histogram (0-23 hours)
commitTimes = commits.map(c => hour(c.committedDate))
workingWindow = findWindow(commitTimes, 80%) // Where 80% commits are

Flag if >10% commits outside working window
Penalty: -5 points for irregular patterns
```

**Red flags:**
- âŒ Commits every day at exactly 8:00 AM â†’ bot pattern
- âœ… Commits scattered 9 AM - 6 PM â†’ normal human pattern
- âš ï¸ Sudden timezone change (was 9-17 UTC, became 2-5 UTC) â†’ suspicious

---

#### C. Collaboration (0-20 points)

**What it measures:** Teamwork through PR reviews, issue participation, discussions.

**Why it matters:** Senior developers spend 30-50% of time on code reviews.

**Calculation:**
```typescript
reviewsDone = count substantive PR reviews (last 6 months)
issuesParticipated = count unique issues with comments
prDiscussionsAvg = avg comments per own PR

Scoring:
â€¢ Reviews:     20+ reviews â†’ 10 points
â€¢ Issues:      10+ issues participated â†’ 5 points
â€¢ Discussions: 3+ avg comments per PR â†’ 5 points
```

**Anti-fake protection:**
- Filter out "LGTM" only reviews (less than 10 characters)
- Filter out "me too" only comments

**Note:** Solo coders will score 0 here â€” that's OK for freelancers!

---

#### D. Project Focus (0-20 points)

**What it measures:** Specialization vs scatter across repositories.

**The Paradox:**
```
1 repo     = excellent (deep focus)
2-5 repos  = ideal (perfect balance)
6-10 repos = good (wide range)
11-20 repos = suspicious (scattered)
20+ repos = red flag (likely fake or bot)
```

**Calculation:**
```typescript
activeRepos = count repos with commits in last 3 months

Scoring:
â€¢ 2-5 repos   â†’ 20 points (ideal balance)
â€¢ 1 repo      â†’ 15 points (deep focus)
â€¢ 6-10 repos  â†’ 12 points (wide range)
â€¢ 11-20 repos â†’ 5 points (scattered)
â€¢ 20+ repos   â†’ 0 points (suspicious)

Penalties:
â€¢ -5 points if >50% repos are unmodified forks
â€¢ -5 points if >70% repos created on same day
```

### Benchmark Ranges

| Score | Level | Interpretation | Hiring Decision |
|-------|-------|----------------|-----------------|
| 0-39 | Low | Inactive or inconsistent work | âš ï¸ Concern |
| 40-59 | Moderate | Regular contributor | âœ… Consider |
| 60-79 | High | Strong consistent productivity | â­ Strong candidate |
| 80-100 | Very High | Elite developer productivity | ðŸŒŸ Excellent |

### Example Output

```
Activity Score: 85/100 (High) âœ…

Breakdown:
â”œâ”€ Code Throughput:      35/35 (12,500 lines/month)
â”œâ”€ Consistency & Rhythm: 25/25 (48 weeks active, 32-week streak)
â”œâ”€ Collaboration:        18/20 (25 reviews, 12 issues)
â””â”€ Project Focus:        20/20 (4 active repos, balanced)

Fraud Risk: 12% (Low) âœ…
```

---

## ðŸŒŸ METRIC 2: Impact Score â†’ Ecosystem Reach

### Purpose

Measures **whether people actually use the developer's work**, not just stars (which can be bought).

**Range:** 0-100 points

### Formula

```
Impact = Adoption Signal (40) +
         Community Engagement (30) +
         Social Proof (20) +
         Package Registry Stats (10)
```

### Components

#### A. Adoption Signal (0-40 points)

**What it measures:** Real usage indicators.

**Calculation:**
```typescript
activeForks = forks with commits ahead of parent (real modifications)
watchers = people following repo updates
contributors = developers who committed
recentActivity = pushed in last 30 days

Score =
  log10(activeForks + 1) Ã— 5 +
  log10(watchers + 1) Ã— 3 +
  min(contributors Ã— 0.5, 10) +
  (recentActivity ? 5 : 0)
```

**Anti-fake protection:**
- Check if forks have actual commits (not just empty forks)
- Verify issues/PRs exist (sign of live project)

**Why active forks matter:** 100 forks with 0 changes = nobody actually uses it. 10 forks with 50+ commits each = real adoption.

---

#### B. Community Engagement (0-30 points)

**What it measures:** Live community activity.

**Calculation:**
```typescript
totalIssues = sum of all issues across repos
closedIssues = issues marked as closed
closureRate = closedIssues / totalIssues
externalPRs = PRs from contributors (not repo owner)

Scoring:
â€¢ Issues (0-15 points):
  - 50+ issues + >50% closed â†’ 15 points
  - 10-50 issues â†’ 10 points
  - <10 issues but >80% closed â†’ 8 points (small but responsive)
  - <10 issues + <50% closed â†’ 3 points

â€¢ External PRs (0-10 points):
  - 20+ external PRs â†’ 10 points
  - Proportional scaling

â€¢ Discussions (0-5 points):
  - GitHub Discussions/Wiki activity
```

**Why issues matter:**
- 0 issues â‰  perfect code
- 0 issues = either nobody uses it OR maintainer ignores them

---

#### C. Social Proof (0-20 points)

**What it measures:** Stars and visibility (with logarithmic scale to prevent gaming).

**Calculation:**
```typescript
starsScore = min(log10(totalStars + 1) Ã— 3, 15)
trendingBonus = wasInTrending ? 5 : 0

Total = starsScore + trendingBonus
```

**Why logarithmic scale:**
```
10 stars â†’ 1,000 stars = +2 points
1,000 stars â†’ 10,000 stars = +2 points (not +9000)
```

This prevents star farming from having outsized impact.

---

#### D. Package Registry Stats (0-10 points)

**What it measures:** Real package downloads from npm/PyPI/crates.io/Docker Hub.

**MVP Implementation:**
```typescript
hasPackageJson = repo has package.json â†’ 5 points
hasDownloadStats = can fetch npm/PyPI stats â†’ +5 points
```

**âš ï¸ DEFERRED TO PHASE 5+** â€” Requires external API calls to npm, PyPI, etc.

### Benchmark Ranges

| Score | Level | Interpretation | Hiring Decision |
|-------|-------|----------------|-----------------|
| 0-19 | None | No community presence | âš ï¸ Junior |
| 20-39 | Local | Small personal projects | âœ… Mid-level potential |
| 40-59 | Community | Active in OSS community | â­ Mid to Senior |
| 60-79 | Regional | Recognized in ecosystem | ðŸŒŸ Senior |
| 80-100 | Global | Industry-wide impact | ðŸ’Ž Staff/Principal |

### Example Output

```
Impact Score: 72/100 (Regional) â­

Top Projects:
1. awesome-lib (75 impact points)
   2,450 stars, 350 active forks
   120 contributors, 850 issues (70% closed)
   50K npm downloads/month

2. useful-tool (62 impact points)
   850 stars, 80 active forks
   25 contributors, maintained (pushed 3 days ago)

Breakdown:
â”œâ”€ Adoption Signal:      38/40 (350 active forks, 2.5K watchers)
â”œâ”€ Community Engagement: 28/30 (850 issues, 70% closed, 45 ext PRs)
â”œâ”€ Social Proof:         18/20 (3,300 total stars, was trending)
â””â”€ Package Stats:        5/10 (publishable, stats pending)
```

---

## ðŸ† METRIC 3: Quality Score â†’ Engineering Maturity

### Purpose

Measures **engineering maturity** through code health practices, not "originality" (which is impossible to measure accurately).

**Range:** 0-100 points

### Formula

```
Quality = Code Health Practices (35) +
          Documentation Quality (25) +
          Maintenance Signal (25) +
          Architecture Complexity (15)
```

### Components

#### A. Code Health Practices (0-35 points)

**What it measures:** Modern engineering habits.

**Calculation:**
```typescript
Scoring:
â€¢ CI/CD (0-15 points):
  - GitHub Actions / CircleCI / Travis exists
  - Automated tests run in CI
  - (repos with CI/CD / total repos) Ã— 15

â€¢ Testing (0-10 points):
  - Test directory exists (test/, tests/, __tests__)
  - Test frameworks in dependencies
  - (repos with tests / total) Ã— 10

â€¢ Linting/Formatting (0-5 points):
  - .eslintrc, .prettierrc, pyproject.toml
  - Pre-commit hooks
  - (repos with linting / total) Ã— 5

â€¢ Code Review Process (0-5 points):
  - Branch protection rules enabled
  - Required reviewers configured
  - (protected repos / total) Ã— 5
```

**How to check:** Use GitHub GraphQL `repository.object(expression: "HEAD:")` to get file tree in one request.

---

#### B. Documentation Quality (0-25 points)

**What it measures:** How well projects are documented.

**Calculation:**
```typescript
Per repository:

README (0-15 points):
â€¢ Length score (0-8):
  - >5000 chars â†’ 8 points (comprehensive)
  - 2000-5000 â†’ 6 points (detailed)
  - 500-2000 â†’ 4 points (basic)
  - <500 â†’ 2 points (minimal)

â€¢ Content score (0-7):
  - Has "Installation" section â†’ 2 points
  - Has "Usage" section â†’ 2 points
  - Has "Examples" â†’ 1 point
  - Has "Contributing" â†’ 1 point
  - Has "License" â†’ 1 point

Wiki (0-5 points):
â€¢ GitHub Wiki enabled with content â†’ 5 points

Docs Site (0-5 points):
â€¢ GitHub Pages / dedicated docs website â†’ 5 points
```

**Anti-fake protection:**
- Flag if README >90% copy of another project (fuzzy string matching)

---

#### C. Maintenance Signal (0-25 points)

**What it measures:** Responsiveness and project longevity.

**Calculation:**
```typescript
â€¢ Issue Response Time (0-10 points):
  medianResponseTime = median hours from issue creation to first response

  Scoring:
  - <24h â†’ 10 points
  - 1-3 days â†’ 7 points
  - 1 week â†’ 4 points
  - >1 month â†’ 0 points

â€¢ Issue Resolution Rate (0-10 points):
  closureRate = closed issues / total issues

  Scoring:
  - >70% â†’ 10 points
  - 50-70% â†’ 7 points
  - <50% â†’ 3 points

â€¢ Project Longevity (0-5 points):
  ageInYears = years since repo creation
  recentActivity = pushed in last 90 days

  Scoring:
  - Age >2 years + active â†’ 5 points
  - Old but abandoned â†’ 0 points
```

**Why this matters:** Shows maintainer doesn't abandon projects and is responsive to community.

---

#### D. Architecture Complexity (0-15 points)

**What it measures:** Technical depth and scale.

**Calculation:**
```typescript
â€¢ Project Size (0-5 points):
  avgDiskUsage = average disk usage across repos

  Scoring:
  - >10MB â†’ 5 points (substantial)
  - 1-10MB â†’ 3 points (moderate)
  - <1MB â†’ 1 point (small)

â€¢ Tech Stack Diversity (0-5 points):
  uniqueLanguages = count languages with >1% of codebase

  Scoring:
  - 5+ languages â†’ 5 points (polyglot)
  - 3-4 languages â†’ 3 points (diverse)
  - 1-2 languages â†’ 1 point (focused)

â€¢ Infrastructure (0-5 points):
  - Has Dockerfile/docker-compose â†’ 3 points
  - Has database migrations â†’ 1 point
  - Has API docs (Swagger/OpenAPI) â†’ 1 point
```

**Anti-fake protection:**
- If many languages but 99% one language â†’ don't count
- Check for real code vs just config files

### Benchmark Ranges

| Score | Level | Interpretation | Hiring Decision |
|-------|-------|----------------|-----------------|
| 0-39 | Beginner | Basic or learning projects | âš ï¸ Junior |
| 40-59 | Intermediate | Decent practices | âœ… Mid-level |
| 60-74 | Advanced | Strong engineering | â­ Senior |
| 75-89 | Expert | Excellent practices | ðŸŒŸ Staff |
| 90-100 | Master | Industry-leading quality | ðŸ’Ž Principal |

### Example Output

```
Quality Score: 78/100 (Expert) ðŸŒŸ

Breakdown:
â”œâ”€ Code Health:      32/35 (CI/CD: 95%, Tests: 90%, Linting: 85%)
â”œâ”€ Documentation:    22/25 (Avg README: 4200 chars, 8 wikis, 3 docs sites)
â”œâ”€ Maintenance:      20/25 (Response: 18h median, Resolution: 72%)
â””â”€ Architecture:     12/15 (Avg: 8.5MB, 4 languages, Docker: 80%)
```

---

## ðŸ“ˆ METRIC 4: Growth Score â†’ Learning Trajectory

### Purpose

Measures **whether the developer is growing as an engineer**, not just whether commit count increases.

**Range:** -100 to +100 points

### Formula

```
Growth = Skill Expansion (40) +
         Project Evolution (30) +
         Learning Pattern Detection (30)
```

### Components

#### A. Skill Expansion (0-40 points)

**What it measures:** New technologies learned.

**Calculation:**
```typescript
recentLanguages = unique languages in repos created last 2 years
olderLanguages = unique languages in repos created 3-5 years ago
newLanguages = recentLanguages - olderLanguages

score = min(newLanguages.size Ã— 10, 40)
```

**Examples:**
- Was only JavaScript, added TypeScript + Rust = +20 points
- Was Python, added Go + Kubernetes = +20 points
- Same Java for 10 years = 0 points (stagnation)

---

#### B. Project Evolution (-30 to +30 points)

**What it measures:** Growth in project complexity over time.

**Calculation:**
```typescript
recentProjects = repos created in last 2 years
olderProjects = repos created 3-5 years ago

complexityScore(repo) =
  stars Ã— 2 +
  forks Ã— 3 +
  diskUsage / 1000 +
  languages Ã— 5 +
  (hasCICD ? 20 : 0)

recentAvg = avg complexity of recent projects
olderAvg = avg complexity of older projects

growthRate = ((recentAvg - olderAvg) / olderAvg) Ã— 100%
score = max(-30, min(growthRate / 3, 30))
```

**Examples:**
- 2020: tutorial projects, 0 stars â†’ 2024: production apps, 100 stars = +30 points
- 2020: 500 stars â†’ 2024: 50 stars = -20 points (declining)

---

#### C. Learning Pattern Detection (0-30 points)

**What it measures:** Balance between learning (tutorials) and shipping (production).

**Tutorial Project Detection:**
```typescript
isTutorial(repo) =
  name includes 'tutorial', 'learning', 'course', 'homework', 'practice' OR
  description includes 'learning', 'following tutorial' OR
  (0 stars AND 0 forks AND abandoned after 2 weeks)
```

**Production Project Detection:**
```typescript
isProduction(repo) =
  stars > 10 OR
  forks > 3 OR
  has CI/CD OR
  issues > 5 OR
  contributors > 3 OR
  pushed in last 90 days
```

**Scoring:**
```typescript
tutorialRatio = tutorial projects / total
productionRatio = production projects / total

Ideal balance:
â€¢ 20% tutorial (experimenting)
â€¢ 60% production (shipping value)
â€¢ 20% abandoned (normal)

Scoring:
â€¢ 100% tutorials â†’ 0 points (only learning, not shipping)
â€¢ 100% production â†’ 20 points (shipping, but not experimenting)
â€¢ 15-25% tutorials + 50-70% production â†’ 30 points (ideal!)
```

### Benchmark Ranges

| Score | Trend | Interpretation | Hiring Decision |
|-------|-------|----------------|-----------------|
| -100 to -30 | Declining | Skills/projects deteriorating | ðŸ”´ Concern |
| -30 to +30 | Stable | Maintaining current level | âš ï¸ Monitor |
| +30 to +70 | Growing | Actively learning & improving | â­ Strong |
| +70 to +100 | Accelerating | Rapid skill development | ðŸŒŸ Excellent |

### Example Output

```
Growth Score: +75/100 (Accelerating) ðŸš€

Timeline:
2020: JavaScript only, tutorial projects (complexity: 15)
2021: +TypeScript, first production app (complexity: 45)
2022: +Rust, contributing to OSS (complexity: 85)
2023: +Go, maintaining 3 production apps (complexity: 120)
2024: +Kubernetes, teaching others (complexity: 180)

Breakdown:
â”œâ”€ Skill Expansion:      40/40 (4 new languages in 2 years)
â”œâ”€ Project Evolution:    28/30 (+650% complexity growth)
â””â”€ Learning Pattern:     30/30 (18% tutorials, 65% production)

Balance: Ideal (actively learning while shipping) âœ…
```

---

## ðŸŽ–ï¸ OVERALL RANK

### Overall Score Formula

```typescript
Overall =
  Activity Ã— 0.25 +
  Impact Ã— 0.30 +
  Quality Ã— 0.30 +
  max(0, Growth) Ã— 0.15

Note: Growth can be negative, but we use max(0, Growth) so
declining growth doesn't penalize too harshly.
```

### Rank Classification

| Rank | Overall Score | Requirements | Expected Level |
|------|---------------|--------------|----------------|
| **Junior** | 0-29 | Low activity, little experience | Entry-level |
| **Mid** | 30-49 | Regular work, some projects | 2-4 years exp |
| **Senior** | 50-69 | High Quality (>60), Impact (>40) | 5-8 years exp |
| **Staff** | 70-84 | Senior + High Impact (>70) | 8-12 years exp |
| **Principal** | 85-100 | Staff + Very High Activity (>70) + Global Impact (>80) | 12+ years exp |

### Example Rankings

**Example 1: Linus Torvalds**
```
Activity:  85 (High, delegates much work)
Impact:    100 (Linux kernel â€” billions of devices)
Quality:   95 (Expert maintainer, 30+ year project)
Growth:    +40 (Stable, mature)

Overall = 85Ã—0.25 + 100Ã—0.30 + 95Ã—0.30 + 40Ã—0.15
        = 21.25 + 30 + 28.5 + 6
        = 85.75

â†’ Rank: Principal ðŸ’Ž
```

**Example 2: Growing Junior Developer**
```
Activity:  45 (Moderate consistency, learning)
Impact:    15 (Local projects only, <100 stars)
Quality:   35 (Basic practices, minimal docs)
Growth:    +60 (Learning fast! 3 new languages)

Overall = 45Ã—0.25 + 15Ã—0.30 + 35Ã—0.30 + 60Ã—0.15
        = 11.25 + 4.5 + 10.5 + 9
        = 35.25

â†’ Rank: Mid âœ… (but growing rapidly â€” worth investing in!)
```

**Example 3: Experienced Mid-Level**
```
Activity:  62 (High, consistent contributor)
Impact:    48 (Active OSS contributor, 1K+ stars)
Quality:   58 (Good CI/CD, decent docs)
Growth:    +25 (Stable, slow growth)

Overall = 62Ã—0.25 + 48Ã—0.30 + 58Ã—0.30 + 25Ã—0.15
        = 15.5 + 14.4 + 17.4 + 3.75
        = 51.05

â†’ Rank: Senior â­
```

---

## ðŸ“ What to Include in MVP (Phase 2)

### âœ… Include (Core Features):

**Activity (35 points):**
- âœ… Code throughput (lines changed in merged PRs)
- âœ… Consistency (3mo, 12mo, 3yr windows)
- âœ… Collaboration (PR reviews, issue participation)
- âœ… Project focus (active repos count)
- âœ… Fraud detection: empty commits, backdating, multiple emails

**Impact (30 points):**
- âœ… Stars + Forks (logarithmic scale)
- âœ… Watchers
- âœ… Contributors count
- âœ… Issues activity (total, closure rate)
- âš ï¸ Package stats (if package.json exists) â€” simplified

**Quality (30 points):**
- âœ… CI/CD presence (GitHub Actions detection)
- âœ… Test directory detection
- âœ… README quality scoring (length + content)
- âœ… Issue response time (median hours)

**Growth (5 points, bonus):**
- âœ… New languages (last 2 years vs 3-5 years ago)
- âœ… Tutorial vs production detection (weighted)
- âœ… Project complexity growth (year-over-year)

---

## âŒ Defer to Phase 5+ (Advanced Features):

- âŒ **Active forks analysis** â€” Requires fetching each fork's commit history (complex, slow)
- âŒ **Bot pattern detection** â€” Diminishing returns, temporal analysis covers most cases
- âŒ **Dependency Graph API** â€” Unstable GitHub API endpoint
- âŒ **Code similarity detection** â€” Overkill for MVP, computationally expensive
- âŒ **Full package registry stats** â€” Requires external API calls to npm, PyPI, cargo, Docker Hub

---

## ðŸ”§ GraphQL Data Requirements

### New Fields Needed (vs Current Implementation)

Current `GET_USER_INFO` query **does NOT include:**

âŒ **For Activity:**
- PR additions/deletions (for Code Throughput)
- PR review comments (for Collaboration)
- Issue comments by user (for Collaboration)
- Commit timestamps with `occurredAt` (for Temporal Pattern Analysis)

âŒ **For Fraud Detection:**
- Commit `additions` and `deletions` (for Empty Commits detection)
- Commit `authoredDate` vs `committedDate` (for Backdating)
- Commit author email addresses (for Multiple Emails)

âŒ **For Quality:**
- Repository file tree (for CI/CD, test detection)
- Issue timeline events (for Response Time)
- Branch protection rules (for Code Review Process)

### Required GraphQL Updates (Phase 1.5)

**New query:** `GET_USER_ANALYTICS` with:

```graphql
pullRequests(first: 100) {
  nodes {
    additions    # NEW
    deletions    # NEW
    merged       # NEW
    reviews {    # NEW
      nodes {
        body
        author { login }
      }
    }
  }
}

commitContributions {
  nodes {
    occurredAt          # NEW (for temporal analysis)
    commitCount
    repository {
      defaultBranchRef {
        target {
          ... on Commit {
            additions   # NEW
            deletions   # NEW
            author {
              email     # NEW
            }
          }
        }
      }
    }
  }
}

repositories {
  nodes {
    object(expression: "HEAD:") {  # NEW (file tree)
      ... on Tree {
        entries {
          name
          type
        }
      }
    }
    branchProtectionRules {  # NEW
      totalCount
    }
    issues {
      nodes {
        timelineItems(first: 1, itemTypes: [ISSUE_COMMENT]) {  # NEW
          nodes {
            ... on IssueComment {
              createdAt
            }
          }
        }
      }
    }
  }
}
```

---

**Last Updated:** 2025-11-17
**Version:** 2.0
**Status:** Ready for Implementation

For detailed TypeScript implementations, see [METRICS_V2_DETAILED.md](./METRICS_V2_DETAILED.md).
